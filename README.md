# ğŸ§  Entrenador Virtual Inteligente

Este proyecto desarrolla un agente conversacional que funciona como entrenador virtual, capaz de generar y ajustar rutinas de entrenamiento funcional segÃºn las necesidades del usuario, combinando NLP, recuperaciÃ³n aumentada por generaciÃ³n (RAG) e interacciÃ³n en tiempo real.

---
# ğŸ—ï¸ Infraestructura y arquitectura
El sistema se estructura en cinco componentes principales:

### 1. Fuente de datos:
Documentos en PDF relacionados con entrenamiento funcional y fisioterapia que conforman la base de conocimiento del agente.
### 2. Pipeline RAG: 
Procesamiento de los documentos mediante tÃ©cnicas de chunking, generaciÃ³n de embeddings y construcciÃ³n de un Ã­ndice semÃ¡ntico para recuperaciÃ³n de informaciÃ³n relevante.
### 3. Modelo conversacional: 
Un agente basado en un modelo de lenguaje (GPT-4o) que interactÃºa con los usuarios, combinando contexto y resultados del motor de recuperaciÃ³n.
### 4. EvaluaciÃ³n automatizada: 
MÃ³dulo que prueba el rendimiento del sistema con preguntas reales y mide la calidad de las respuestas generadas usando mÃ©tricas automÃ¡ticas.
### 5. Interfaz de usuario: 
AplicaciÃ³n web interactiva desarrollada en Streamlit para facilitar la conversaciÃ³n con el agente y visualizar mÃ©tricas de rendimiento.

---

# âš™ï¸ Herramientas principales
- **LangChain** para la construcciÃ³n del pipeline RAG y la orquestaciÃ³n del agente.
- **OpenAI GPT-4o** como modelo de lenguaje principal.
- **FAISS** para almacenamiento y bÃºsqueda eficiente de vectores semÃ¡nticos.
- **Streamlit** para la interfaz interactiva.
- **MLflow** para el tracking de experimentos y visualizaciÃ³n de mÃ©tricas.
- **Supabase** para gestiÃ³n de datos adicionales y posibles registros de retroalimentaciÃ³n del usuario.
- **dotenv** y **os** para la gestiÃ³n de variables de entorno.

# ğŸ“Š EvaluaciÃ³n del sistema
El sistema incluye un mÃ³dulo de evaluaciÃ³n automatizada que permite medir la calidad de las respuestas del agente. A travÃ©s de un conjunto de preguntas de prueba, se evalÃºa su desempeÃ±o en criterios como precisiÃ³n, completitud y coherencia usando el mÃ³dulo LabeledCriteriaEvalChain de LangChain. Los resultados se registran automÃ¡ticamente en MLflow y pueden visualizarse mediante dashboard.


# ğŸ§ª CÃ³mo ejecutar
### 1. Clona el repositorio:

```bash
git clone https://github.com/drangelv/chat_final.git
cd nlp_u
```

### 2. Crea un entorno virtual y activa:

```bash
python -m venv venv
source venv/bin/activate
```

### 3. Instala las dependencias:

```bash
pip install -r requirements.txt
```

### 4. Define tus variables de entorno en un archivo .env:

```bash
cp .env.example .env  # Agrega tus API KEY de OpenAI y Supabase
```


OPENAI_API_KEY=tu_api_key

SUPABASE_URL=tu_url

SUPABASE_KEY=tu_clave


### 5. Crear base de datos vectorial:

```bash
python -c "from app.rag_pipeline import save_vectorstore; save_vectorstore()"
```

Para reutilizarlo directamente:
```python
vectordb = load_vectorstore_from_disk()
```

### 6. Ejecuta la interfaz:
Chat de entrenamiento
```bash
streamlit run app/ui_streamlit.py
```

### 7. EvaluaciÃ³n:

```bash
python app/run_eval.py
```

MÃ©tricas de evaluaciÃ³n:
```bash
streamlit run app/eval_streamlit.py
```


## ğŸ“‚ Estructura del Proyecto

```
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ ui_streamlit.py           â† interfaz simple del chatbot
â”‚   â”œâ”€â”€ eval_streamlit.py         â† interfaz combinada con mÃ©tricas
â”‚   â”œâ”€â”€ supabase_client.py        â† Almacenamiento y autenticaciÃ³n de BD
â”‚   â”œâ”€â”€ run_eval.py               â† evaluaciÃ³n automÃ¡tica
â”‚   â”œâ”€â”€ rag_pipeline.py           â† lÃ³gica de ingestiÃ³n y RAG
â”‚   â””â”€â”€ prompts/
â”‚       â”œâ”€â”€ v1_asistente_entrenamiento.txt
â”‚       â””â”€â”€ v2_resumido_directo.txt
â”œâ”€â”€ data/pdfs/                    â† documentos fuente
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_run_eval.py
â”‚   â”œâ”€â”€ eval_dataset.json         â† dataset de evaluaciÃ³n
â”‚   â””â”€â”€ eval_dataset.csv
â”œâ”€â”€ .env.example
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ .devcontainer/
â”‚   â””â”€â”€ devcontainer.json
â”œâ”€â”€ .github/workflows/
â”‚   â”œâ”€â”€ eval.yml
â”‚   â””â”€â”€ test.yml    

---
